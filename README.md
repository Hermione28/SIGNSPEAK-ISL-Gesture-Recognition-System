# SignSpeak â€“ ISL Gesture Recognition System 



## **Description**

This project aims to detect and recognize Indian Sign Language (ISL) gestures using **Mediapipe**, **OpenCV**, and **Machine Learning**. The system extracts hand landmarks in real time and classifies them using a trained neural network. The repository includes code for dataset preprocessing, keypoint extraction, model training, and real-time ISL gesture prediction.

---

## **Overview**

**Dataset:** Indian Sign Language Dataset â€“ Kaggle
**Programming Language:** Python
**Libraries Used:** Mediapipe, OpenCV, NumPy, TensorFlow/Keras

**Dataset Link:**
[https://www.kaggle.com/datasets/prathumarikeri/indian-sign-language-isl](https://www.kaggle.com/datasets/prathumarikeri/indian-sign-language-isl)

---
**Project Structure**

SignSpeak/
â”œâ”€â”€ dataset/                         # Raw ISL gesture images
â”œâ”€â”€ images/                          # Sample images for reference
â”œâ”€â”€ keypoint.csv                     # Extracted 42 hand landmarks in CSV format

â”œâ”€â”€ ISL_classifier.ipynb             # Model training Jupyter notebook
â”œâ”€â”€ model.h5                         # Trained gesture recognition model

â”œâ”€â”€ isl_detection.py                 # Real-time detection and prediction script
â”œâ”€â”€ dataset_keypoint_generation.py   # Script to generate hand keypoints from dataset

â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ README.md                        # Project documentation
---

## **How It Works**

The system uses **Mediapipe Hands** to detect hand and finger landmarks from webcam input in real time. These extracted **42 keypoints** are fed into a trained **feedforward neural network (FNN)**, which predicts the ISL gesture class.

**Workflow:**

1. Webcam captures a live video frame.
2. Mediapipe detects the hand and extracts 21 keypoints per hand.
3. Extracted coordinates are normalized and passed to the trained classifier.
4. The model predicts the gesture class.
5. The predicted result is displayed on the video stream in real time.

image (gesture process)

---

## **Requirements**

* Python 3.6 or higher
* Mediapipe
* OpenCV
* Numpy
* TensorFlow / Keras

---

## **Installation**

1. Install Python (3.6+).
2. Run the following commands:

```
pip install mediapipe
pip install opencv-python
pip install numpy
pip install tensorflow
```

---

## **Usage**

1. Clone the repository.
2. Open a terminal in the project directory.
3. Run the real-time detection script:

```
python isl_detection.py
```

4. Press **â€˜qâ€™** to exit the program.

---

## **Examples**

![Example1](ASSETS/Example1.png)
![Example2](ASSETS/Example2.png)

---

## **Next Steps**

âš™ï¸ **Accuracy Improvement:**
Experiment with CNN, LSTM, or hybrid models to improve classification accuracy.

ğŸ“‚ **Dataset Expansion:**
Add more samples and new gesture categories.

ğŸ”Š **Speech/Text Output:**
Convert recognized gestures into text or speech for communication support.

ğŸ–¥ï¸ **GUI Integration:**
Add a user-friendly interface for real-time interaction.

ğŸš€ **Model Deployment:**
Deploy the system as a web application or Android app using TensorFlow Lite.

ğŸ¤ **Contributions:**
Fork the repository, create a new branch, and submit a pull request.
Issues can be opened for bugs, enhancements, or new features.

---

## **Acknowledgments**

* Dataset sourced from **Kaggle â€“ Indian Sign Language Dataset**.
* Thanks to the Mediapipe and TensorFlow teams for powerful open-source tools.

---

## **Author**

Prajakta Jagdale

[LinkedIn](www.linkedin.com/in/prajakta-jagdale-665a0a257)

[GitHub](https://github.com/Hermione28)

---

## **About**

A real-time ISL detection system developed using **Mediapipe** and **Machine Learning**.
Includes dataset processing, landmark extraction, model training, and real-time prediction â€” useful for gesture recognition, accessibility tools, and ISL communication.

---

